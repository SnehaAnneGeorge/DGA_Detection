{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a19d65f-520b-49fb-b6f9-8f60aed6bd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import joblib\n",
    "import tldextract\n",
    "\n",
    "# Function to load the file based on user input\n",
    "def load_file(filename):\n",
    "if filename.endswith('.csv'):\n",
    "df = pd.read_csv(filename)\n",
    "elif filename.endswith('.xlsx'):\n",
    "df = pd.read_excel(filename)\n",
    "else:\n",
    "raise ValueError(\"Unsupported file format. Please upload a CSV or Excel file.\")\n",
    "\n",
    "return df\n",
    "\n",
    "# Function to extract features from the domain column\n",
    "def extract_features(df, domain_column='domain'):\n",
    "# Ensure all values in the domain column are strings\n",
    "df[domain_column] = df[domain_column].astype(str).fillna('')\n",
    "\n",
    "# Extract components of the domain\n",
    "extracted = df[domain_column].apply(tldextract.extract)\n",
    "df['subdomain'] = extracted.apply(lambda x: x.subdomain)\n",
    "df['domain'] = extracted.apply(lambda x: x.domain)\n",
    "df['suffix'] = extracted.apply(lambda x: x.suffix)\n",
    "\n",
    "# Fill missing values with empty strings\n",
    "df['subdomain'] = df['subdomain'].fillna('')\n",
    "df['domain'] = df['domain'].fillna('')\n",
    "df['suffix'] = df['suffix'].fillna('')\n",
    "\n",
    "# Create new features\n",
    "df['domain_length'] = df['domain'].apply(len)\n",
    "df['domain_num_digits'] = df['domain'].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "df['domain_num_special_chars'] = df['domain'].apply(lambda x: sum(not c.isalnum() for c\n",
    "in x))\n",
    "df['domain_num_subdomains'] = df['subdomain'].apply(lambda x: len(x.split('.')))\n",
    "df['domain_entropy'] = df['domain'].apply(lambda x: -sum(p * np.log2(p) for p in\n",
    "pd.Series(list(x)).value_counts(normalize=True)))\n",
    "\n",
    "df['host'] = df['subdomain'] + '.' + df['domain'] + '.' + df['suffix']\n",
    "df['host_length'] = df['host'].apply(len)\n",
    "df['host_num_digits'] = df['host'].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "df['host_num_special_chars'] = df['host'].apply(lambda x: sum(not c.isalnum() for c in x))\n",
    "df['host_num_subdomains'] = df['subdomain'].apply(lambda x: len(x.split('.')))\n",
    "df['host_entropy'] = df['host'].apply(lambda x: -sum(p * np.log2(p) for p in\n",
    "pd.Series(list(x)).value_counts(normalize=True)))\n",
    "return df\n",
    "\n",
    "# Get the filename from the user for training\n",
    "filename = input(\"Enter the name of the training file (with extension) to load: \").strip()\n",
    "try:\n",
    "df = load_file(filename)\n",
    "print(\"File loaded successfully!\")\n",
    "display(df.head()) # Display the first few rows of the DataFrame\n",
    "except Exception as e:\n",
    "print(e)\n",
    "\n",
    "# Check if the DataFrame is loaded\n",
    "if 'df' in locals():\n",
    "# Extract features\n",
    "df = extract_features(df)\n",
    "\n",
    "# Define features and target variable\n",
    "X = df.drop(columns=['isDGA', 'subdomain', 'domain', 'suffix', 'host'])\n",
    "y = df['isDGA'].map({'dga': 1, 'legit': 0})\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=33)\n",
    "\n",
    "# Define the preprocessing steps\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "('imputer', SimpleImputer(strategy='median') ])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "transformers=[\n",
    "('num', numeric_transformer, numeric_features) ])\n",
    "\n",
    "# Define the model\n",
    "model = DecisionTreeClassifier(random_state=33)\n",
    "\n",
    "# Create and train the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "('preprocessor', preprocessor),\n",
    "('classifier', model) ])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model accuracy: {accuracy}\")\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred, target_names=['non-dga', 'dga']))\n",
    "\n",
    "# Print the number of detected DGA and non-DGA URLs\n",
    "dga_count = sum(y_pred == 1)\n",
    "non_dga_count = sum(y_pred == 0)\n",
    "print(f\"Number of detected DGA URLs: {dga_count}\")\n",
    "print(f\"Number of detected non-DGA URLs: {non_dga_count}\")\n",
    "\n",
    "# Save the trained model pipeline\n",
    "joblib.dump(pipeline, 'dga_detection_model.pkl')\n",
    "print(\"Trained model saved as 'dga_detection_model.pkl'\")\n",
    "\n",
    "else:\n",
    "print(\"DataFrame is not loaded. Please upload the file.\")\n",
    "\n",
    "# Function to test the model on a new dataset\n",
    "def test_new_dataset(model_filename, test_filename):\n",
    "try:\n",
    "# Load the trained model\n",
    "pipeline = joblib.load(model_filename)\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Load the test dataset\n",
    "test_df = load_file(test_filename)\n",
    "print(\"Test file loaded successfully!\")\n",
    "display(test_df.head())\n",
    "\n",
    "# Extract features from the test dataset\n",
    "test_df = extract_features(test_df)\n",
    "X_new = test_df.drop(columns=['domain', 'subdomain', 'suffix', 'host']) # Only drop the\n",
    "domain components\n",
    "\n",
    "# Make predictions\n",
    "y_pred_new = pipeline.predict(X_new)\n",
    "\n",
    "# Print the number of detected DGA and non-DGA URLs\n",
    "dga_count_new = sum(y_pred_new == 1)\n",
    "non_dga_count_new = sum(y_pred_new == 0)\n",
    "print(f\"Number of detected DGA URLs: {dga_count_new}\")\n",
    "print(f\"Number of detected non-DGA URLs: {non_dga_count_new}\")\n",
    "\n",
    "# Display predictions\n",
    "predictions = pd.DataFrame({\n",
    "'domain': test_df['domain'] + '.' + test_df['suffix'],\n",
    "'prediction': y_pred_new\n",
    "}).replace({1: 'dga', 0: 'non-dga'})\n",
    "display(predictions)\n",
    "\n",
    "# Save the DGA and non-DGA URLs into separate files\n",
    "dga_urls = predictions[predictions['prediction'] == 'dga']\n",
    "non_dga_urls = predictions[predictions['prediction'] == 'non-dga']\n",
    "\n",
    "dga_urls.to_csv('detected_dga_urls.csv', index=False)\n",
    "non_dga_urls.to_csv('detected_non_dga_urls.csv', index=False)\n",
    "print(\"Detected DGA URLs saved as 'detected_dga_urls.csv'\")\n",
    "print(\"Detected non-DGA URLs saved as 'detected_non_dga_urls.csv'\")\n",
    "\n",
    "except Exception as e:\n",
    "print(e)\n",
    "\n",
    "# Get the test filename from the user\n",
    "test_filename = input(\"Enter the name of the test file (with extension) to load: \").strip()\n",
    "test_new_dataset('dga_detection_model.pkl', test_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
